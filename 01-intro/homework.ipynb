{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71f901a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aeb63b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-01.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4f959af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VendorID</th>\n",
       "      <th>tpep_pickup_datetime</th>\n",
       "      <th>tpep_dropoff_datetime</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>trip_distance</th>\n",
       "      <th>RatecodeID</th>\n",
       "      <th>store_and_fwd_flag</th>\n",
       "      <th>PULocationID</th>\n",
       "      <th>DOLocationID</th>\n",
       "      <th>payment_type</th>\n",
       "      <th>fare_amount</th>\n",
       "      <th>extra</th>\n",
       "      <th>mta_tax</th>\n",
       "      <th>tip_amount</th>\n",
       "      <th>tolls_amount</th>\n",
       "      <th>improvement_surcharge</th>\n",
       "      <th>total_amount</th>\n",
       "      <th>congestion_surcharge</th>\n",
       "      <th>airport_fee</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>2023-01-01 00:32:10</td>\n",
       "      <td>2023-01-01 00:40:36</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.97</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>161</td>\n",
       "      <td>141</td>\n",
       "      <td>2</td>\n",
       "      <td>9.3</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>14.30</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2023-01-01 00:55:08</td>\n",
       "      <td>2023-01-01 01:01:27</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.10</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>43</td>\n",
       "      <td>237</td>\n",
       "      <td>1</td>\n",
       "      <td>7.9</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.5</td>\n",
       "      <td>4.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16.90</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2023-01-01 00:25:04</td>\n",
       "      <td>2023-01-01 00:37:49</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.51</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>48</td>\n",
       "      <td>238</td>\n",
       "      <td>1</td>\n",
       "      <td>14.9</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.5</td>\n",
       "      <td>15.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>34.90</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2023-01-01 00:03:48</td>\n",
       "      <td>2023-01-01 00:13:25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.90</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>138</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>12.1</td>\n",
       "      <td>7.25</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20.85</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>2023-01-01 00:10:29</td>\n",
       "      <td>2023-01-01 00:21:19</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.43</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>107</td>\n",
       "      <td>79</td>\n",
       "      <td>1</td>\n",
       "      <td>11.4</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3.28</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>19.68</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   VendorID tpep_pickup_datetime tpep_dropoff_datetime  passenger_count  \\\n",
       "0         2  2023-01-01 00:32:10   2023-01-01 00:40:36              1.0   \n",
       "1         2  2023-01-01 00:55:08   2023-01-01 01:01:27              1.0   \n",
       "2         2  2023-01-01 00:25:04   2023-01-01 00:37:49              1.0   \n",
       "3         1  2023-01-01 00:03:48   2023-01-01 00:13:25              0.0   \n",
       "4         2  2023-01-01 00:10:29   2023-01-01 00:21:19              1.0   \n",
       "\n",
       "   trip_distance  RatecodeID store_and_fwd_flag  PULocationID  DOLocationID  \\\n",
       "0           0.97         1.0                  N           161           141   \n",
       "1           1.10         1.0                  N            43           237   \n",
       "2           2.51         1.0                  N            48           238   \n",
       "3           1.90         1.0                  N           138             7   \n",
       "4           1.43         1.0                  N           107            79   \n",
       "\n",
       "   payment_type  fare_amount  extra  mta_tax  tip_amount  tolls_amount  \\\n",
       "0             2          9.3   1.00      0.5        0.00           0.0   \n",
       "1             1          7.9   1.00      0.5        4.00           0.0   \n",
       "2             1         14.9   1.00      0.5       15.00           0.0   \n",
       "3             1         12.1   7.25      0.5        0.00           0.0   \n",
       "4             1         11.4   1.00      0.5        3.28           0.0   \n",
       "\n",
       "   improvement_surcharge  total_amount  congestion_surcharge  airport_fee  \n",
       "0                    1.0         14.30                   2.5         0.00  \n",
       "1                    1.0         16.90                   2.5         0.00  \n",
       "2                    1.0         34.90                   2.5         0.00  \n",
       "3                    1.0         20.85                   0.0         1.25  \n",
       "4                    1.0         19.68                   2.5         0.00  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71863f5e",
   "metadata": {},
   "source": [
    "## Q1. Downloading the data\n",
    "We'll use the same NYC taxi dataset, but instead of \"Green Taxi Trip Records\", we'll use \"Yellow Taxi Trip Records\".\n",
    "\n",
    "Download the data for January and February 2023.\n",
    "\n",
    "Read the data for January. How many columns are there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31283f17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many columns are there?: 19\n"
     ]
    }
   ],
   "source": [
    "ncolumns = df.shape[1]\n",
    "print(f\"How many columns are there?: {ncolumns}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd164d68",
   "metadata": {},
   "source": [
    "## Q2. Computing duration\n",
    "Now let's compute the duration variable. It should contain the duration of a ride in minutes.\n",
    "\n",
    "What's the standard deviation of the trips duration in January?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91100786",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['duration'] = df.apply(lambda x: abs( x['tpep_dropoff_datetime'] - x['tpep_pickup_datetime'] ).total_seconds() / 60.0, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2830e23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What's the standard deviation of the trips duration in January?: 42.59\n"
     ]
    }
   ],
   "source": [
    "stdDurJan = round((df['duration'].std()),2)\n",
    "print(f\"What's the standard deviation of the trips duration in January?: {stdDurJan}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "274fbb86",
   "metadata": {},
   "source": [
    "## Q3. Dropping outliers\n",
    "Next, we need to check the distribution of the duration variable. There are some outliers. Let's remove them and keep only the records where the duration was between 1 and 60 minutes (inclusive).\n",
    "\n",
    "What fraction of the records left after you dropped the outliers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1121dfca",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleandDF = df[(df['duration']>= 1.0) & (df['duration']<= 60.0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f2254411",
   "metadata": {},
   "outputs": [],
   "source": [
    "recordsLeft = round((len(cleandDF)/len(df))*100,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d5e0d0b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What fraction of the records left after you dropped the outliers?: 98.0%\n"
     ]
    }
   ],
   "source": [
    "print(f\"What fraction of the records left after you dropped the outliers?: {recordsLeft}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d9b71e",
   "metadata": {},
   "source": [
    "## Q4. One-hot encoding\n",
    "Let's apply one-hot encoding to the pickup and dropoff location IDs. We'll use only these two features for our model.\n",
    "\n",
    "Turn the dataframe into a list of dictionaries (remember to re-cast the ids to strings - otherwise it will label encode them)\n",
    "\n",
    "*Fit a dictionary vectorizer\n",
    "\n",
    "*Get a feature matrix from it\n",
    "\n",
    "What's the dimensionality of this matrix (number of columns)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "92049235",
   "metadata": {},
   "outputs": [],
   "source": [
    "onehotDF = pd.DataFrame()\n",
    "\n",
    "onehotDF['PULocationID'] = cleandDF['PULocationID'].astype(str)\n",
    "onehotDF['DOLocationID'] = cleandDF['DOLocationID'].astype(str)\n",
    "onehotDF['duration'] = cleandDF['duration']\n",
    "onehotDF = onehotDF.drop_duplicates()\n",
    "\n",
    "#onehotDFDict = onehotDF[['PULocationID','DOLocationID']].drop_duplicates().to_dict(orient='records')\n",
    "dataDicts = onehotDF[['PULocationID', 'DOLocationID']].to_dict(orient='records')\n",
    "target = onehotDF['duration'].values\n",
    "\n",
    "del df\n",
    "del cleandDF\n",
    "del onehotDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6f9a1b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dicVec = DictVectorizer(sparse=False)\n",
    "#feature_matrix = dicVec.fit_transform(onehotDFDict)\n",
    "feature_matrix = dicVec.fit_transform(dataDicts)\n",
    "n_feature_cols = feature_matrix.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "23a4d936",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What's the dimensionality of this matrix (number of columns)?: 515\n"
     ]
    }
   ],
   "source": [
    "print(f\"What's the dimensionality of this matrix (number of columns)?: {n_feature_cols}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e428ce66",
   "metadata": {},
   "source": [
    "## Q5. Training a model\n",
    "Now let's use the feature matrix from the previous step to train a model.\n",
    "\n",
    "Train a plain linear regression model with default parameters\n",
    "Calculate the RMSE of the model on the training data\n",
    "What's the RMSE on train?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7bac6f97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE on training data: 11.736663450001835\n"
     ]
    }
   ],
   "source": [
    "# Define the chunk size\n",
    "chunk_size = 100  # You can adjust this according to your dataset size\n",
    "\n",
    "# Initialize the model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Initialize lists to store predictions and targets\n",
    "all_predictions = []\n",
    "all_targets = []\n",
    "\n",
    "# Iterate over the data in chunks\n",
    "for i in range(0, len(feature_matrix), chunk_size):\n",
    "    # Get the chunk of features and targets\n",
    "    X_chunk = feature_matrix[i:i+chunk_size]\n",
    "    y_chunk = target[i:i+chunk_size]\n",
    "\n",
    "    # Train the model on the chunk\n",
    "    model.fit(X_chunk, y_chunk)\n",
    "\n",
    "for i in range(0, len(feature_matrix), chunk_size):\n",
    "    # Get the chunk of features and targets\n",
    "    X_chunk = feature_matrix[i:i+chunk_size]\n",
    "    y_chunk = target[i:i+chunk_size]\n",
    "    \n",
    "    # Predict on the chunk\n",
    "    chunk_predictions = model.predict(X_chunk)\n",
    "    y_chunk = target[i:i+chunk_size]\n",
    "\n",
    "    # Store predictions and targets\n",
    "    all_predictions.extend(chunk_predictions)\n",
    "    all_targets.extend(y_chunk)\n",
    "\n",
    "# Calculate the RMSE\n",
    "rmse = np.sqrt(mean_squared_error(all_targets, all_predictions))\n",
    "print(\"RMSE on training data:\", rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835ab186",
   "metadata": {},
   "source": [
    "## Q6. Evaluating the model\n",
    "Now let's apply this model to the validation dataset (February 2023).\n",
    "\n",
    "What's the RMSE on validation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cb4b02f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "joblib.dump(model, 'linear_regression_model.pkl')\n",
    "\n",
    "# Save the DictVectorizer object\n",
    "joblib.dump(dicVec, 'dict_vectorizer.pkl')\n",
    "\n",
    "\n",
    "%reset -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "16accd97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import joblib\n",
    "\n",
    "def cleaning_df(df):\n",
    "    df['duration'] = df.apply(lambda x: abs( x['tpep_dropoff_datetime'] - x['tpep_pickup_datetime'] ).total_seconds() / 60.0, axis=1)\n",
    "    cleandDF = df[(df['duration']>= 1.0) & (df['duration']<= 60.0)]\n",
    "    onehotDF = pd.DataFrame()\n",
    "\n",
    "    onehotDF['PULocationID'] = cleandDF['PULocationID'].astype(str)\n",
    "    onehotDF['DOLocationID'] = cleandDF['DOLocationID'].astype(str)\n",
    "    onehotDF['duration'] = cleandDF['duration']\n",
    "    onehotDF = onehotDF.drop_duplicates()\n",
    "\n",
    "    #onehotDFDict = onehotDF[['PULocationID','DOLocationID']].drop_duplicates().to_dict(orient='records')\n",
    "    dataDicts = onehotDF[['PULocationID', 'DOLocationID']].to_dict(orient='records')\n",
    "    target = onehotDF['duration'].values\n",
    "\n",
    "    del df\n",
    "    del cleandDF\n",
    "    del onehotDF\n",
    "    \n",
    "    return dataDicts, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "77bfca4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained model\n",
    "model = joblib.load('linear_regression_model.pkl')\n",
    "\n",
    "# Load the DictVectorizer object\n",
    "dicVec = joblib.load('dict_vectorizer.pkl')\n",
    "\n",
    "df_validation = pd.read_parquet(\"https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-02.parquet\")\n",
    "dataDicts, target = cleaning_df(df_validation)\n",
    "del df_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "72492999",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to store predictions and targets\n",
    "all_predictions = []\n",
    "all_targets = []\n",
    "\n",
    "#dicVec = DictVectorizer(sparse=False)\n",
    "#feature_matrix = dicVec.fit_transform(onehotDFDict)\n",
    "feature_matrix = dicVec.transform(dataDicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d0479fa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE on validation data: 11.892177834522558\n"
     ]
    }
   ],
   "source": [
    "chunk_size = 100\n",
    "\n",
    "for i in range(0, len(feature_matrix), chunk_size):\n",
    "    # Get the chunk of features and targets\n",
    "    X_chunk = feature_matrix[i:i+chunk_size]\n",
    "    y_chunk = target[i:i+chunk_size]\n",
    "    \n",
    "    # Predict on the chunk\n",
    "    chunk_predictions = model.predict(X_chunk)\n",
    "    y_chunk = target[i:i+chunk_size]\n",
    "\n",
    "    # Store predictions and targets\n",
    "    all_predictions.extend(chunk_predictions)\n",
    "    all_targets.extend(y_chunk)\n",
    "\n",
    "# Calculate the RMSE\n",
    "rmse = np.sqrt(mean_squared_error(all_targets, all_predictions))\n",
    "print(\"RMSE on validation data:\", rmse)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
